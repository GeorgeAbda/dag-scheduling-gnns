### 3.2. Gradient-based Estimation

We now describe our algorithm, which builds on the above linearization property, by using logistic regression with gradients as features. It also includes dimension reduction, as described below.
(1) Estimating fine-tuned model parameters: In the following discussion, we focus on binary classification, such that yiâˆˆ{+1,âˆ’1}subscriptğ‘¦ğ‘–11y_{i}\in\{+1,-1\}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ { + 1 , - 1 }. See Remark [3.2](https://arxiv.org/html/2409.06091v1) for extensions to multiple classification and regression. Recall the gradient-based approximation of fWâ¢(xi,yi)subscriptğ‘“ğ‘Šsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–f_{W}(x_{i},y_{i})italic_f start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), given the input (xi,yi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–(x_{i},y_{i})( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ):
Let us denote âˆ‡WfÎ¸â‹†â¢(xi,yi)subscriptâˆ‡ğ‘Šsubscriptğ‘“superscriptğœƒâ‹†subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–\nabla_{W}f_{\theta^{\star}}(x_{i},y_{i})âˆ‡ start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) as gisubscriptğ‘”ğ‘–g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and âˆ’yiâ¢fÎ¸â‹†â¢(xi,yi)subscriptğ‘¦ğ‘–subscriptğ‘“superscriptğœƒâ‹†subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–-y_{i}f_{\theta^{\star}}(x_{i},y_{i})- italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) as bisubscriptğ‘ğ‘–b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, for any iğ‘–iitalic_i. Using logistic loss, we can write down the loss function as
for Wâˆˆpsuperscriptğ‘ğ‘ŠabsentW\in^{p}italic_W âˆˆ start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT. Denote the combined data set in the task subset Sğ‘†Sitalic_S as
where nSsubscriptğ‘›ğ‘†n_{S}italic_n start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT is the combined number of data samples in the set ğ’ŸSsubscriptğ’Ÿğ‘†\mathcal{D}_{S}caligraphic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT.
The main idea is to solve a logistic regression problem with gisubscriptğ‘”ğ‘–g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being the feature vector and yisubscriptğ‘¦ğ‘–y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being the response label. However, keep in mind that the dimension of gisubscriptğ‘”ğ‘–g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the same as the number of parameters in a neural network, which could be tens of millions. Thus, we introduce a dimension reduction procedure that does not lose much precision.
(2) Dimension reduction: We use the Johnson-Lindenstrauss random projection (johnson1984extensions), which projects the gradients to a much lower dimension before solving the logistic regression. Let Pğ‘ƒPitalic_P be a pğ‘pitalic_p by dğ‘‘ditalic_d Gaussian random matrix, whose entries are independently sampled from a Gaussian Nâ¢(0,dâˆ’1)ğ‘0superscriptğ‘‘1N(0,d^{-1})italic_N ( 0 , italic_d start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ). We project the gradient from dimension pğ‘pitalic_p onto dimension dğ‘‘ditalic_d as g~i=PâŠ¤â¢gisubscript~ğ‘”ğ‘–superscriptğ‘ƒtopsubscriptğ‘”ğ‘–\tilde{g}_{i}=P^{\top}g_{i}over~ start_ARG italic_g end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_P start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Then, we solve the following logistic regression, which is now in dimension dğ‘‘ditalic_d: